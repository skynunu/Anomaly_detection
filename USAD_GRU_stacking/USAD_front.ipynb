{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etniX_KTlJ5U"
   },
   "source": [
    "# USAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3jM0qLU8MgZ"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjheCL2b1Rnw"
   },
   "outputs": [],
   "source": [
    "!rm -r sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "e3dDxs8LFZdT",
    "outputId": "ebff804d-1c59-4039-d869-f65907b19712"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/manigalati/usad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from contextlib2 import redirect_stdout\n",
    "from utils import *\n",
    "from usad import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATANAME = 'SWaT' # 데이터 이름은 HAI, SWaT, WADI 중에서 선택\n",
    "FILENUM = 'USAD_front'\n",
    "window_size= 12\n",
    "BATCH_SIZE =  2048\n",
    "N_EPOCHS = 100\n",
    "hidden_size = 100\n",
    "alpha=.5 \n",
    "beta =.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4AzWlDBI_djV",
    "outputId": "7a8d0c19-2389-461b-c0be-3427a25dda91"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi -L\n",
    "\n",
    "device = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "    return pd.concat([dataframe_from_csv(x) for x in targets])\n",
    "\n",
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1crx5rGP9ONf"
   },
   "source": [
    "## Data PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxofeE469RhT"
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load & Merge function\n",
    "def data_load_merge (dataname) :\n",
    "    if dataname == 'HAI': # train data에서 80%만 가져온다.\n",
    "        train_data = sorted([x for x in Path(\"data/HAI 2.0/training/\").glob(\"*.csv.gz\")]) \n",
    "        train_data= dataframe_from_csvs(train_data)\n",
    "        length = int(train_data.shape[0]*0.8)\n",
    "        train_data = train_data.iloc[:length]\n",
    "       \n",
    "    elif dataname == 'SWaT' :\n",
    "        train_data = pd.read_csv(\"data/SWaT_Dataset_Normal_v1.csv\")\n",
    "\n",
    "    elif dataname == 'WADI': \n",
    "        train_data = pd.read_csv(\"data/WADI_14days_new.csv\")\n",
    "        length = int(train_data.shape[0]*0.8)\n",
    "        train_data = train_data.iloc[:length]\n",
    "        \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_original = data_load_merge(DATANAME) \n",
    "print(normal_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfSj4FYL9W8Y"
   },
   "source": [
    "### Normal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# HAI data preprocessing\n",
    "if DATANAME == 'HAI' :\n",
    "    columns = normal_original.columns\n",
    "    DROP_FIELD = [\"time\", \n",
    "                columns[2], columns[3], columns[14], columns[18], columns[19], columns[21], columns[22], columns[25], columns[33], columns[34], columns[35], columns[37], columns[40], columns[43], columns[51], columns[52], columns[59], columns[61], columns[63], columns[64], columns[65], columns[67],\n",
    "                columns[4], columns[5], columns[6], columns[7], columns[8], columns[10], columns[11], columns[17], columns[24], columns[28], columns[32], columns[44], columns[46], columns[48], columns[49], columns[50], columns[53], columns[58], columns[62], columns[71], columns[76], columns[78], columns[79]]\n",
    "\n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = normal_original.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
    "    \n",
    "    TAG_MIN = normal_original[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "    TAG_MAX = normal_original[VALID_COLUMNS_IN_TRAIN_DATASET].max()\n",
    "\n",
    "    # Min-Max Normalize\n",
    "    normal = normalize(normal_original[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "\n",
    "# SWaT data preprocessing\n",
    "if DATANAME == 'SWaT' :\n",
    "    columns = normal_original.columns\n",
    "    DROP_FIELD = [\"Timestamp\", \"Normal/Attack\",'P102','P201','P202','P204','P206','P401','P403','P404','P502','P601','P603','P301']\n",
    "  \n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = normal_original.columns.drop(DROP_FIELD)\n",
    "    normal = normal_original[VALID_COLUMNS_IN_TRAIN_DATASET]\n",
    "\n",
    "    # Transform all columns into float64\n",
    "    for i in list( normal): \n",
    "         normal[i]= normal[i].apply(lambda x: str(x).replace(\",\" , \".\"))\n",
    "    normal =  normal.astype(float)\n",
    "\n",
    "    # Transform all columns into float64\n",
    "    x = normal.values\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    normal = pd.DataFrame(x_scaled)\n",
    "\n",
    "# WADI data preprocessing\n",
    "if DATANAME == 'WADI' :\n",
    "    columns = normal_original.columns\n",
    "    DROP_FIELD = [\"Time\", \"Date\",\"Row\", '2_LS_001_AL', '2_LS_002_AL', '2_P_001_STATUS', '2_P_002_STATUS','1_LS_001_AL','1_LS_002_AL','1_MV_002_STATUS','1_MV_003_STATUS','1_P_002_STATUS','1_P_004_STATUS','1_P_006_STATUS','2_P_004_STATUS','2_PIC_003_SP','2_SV_101_STATUS','2_SV_201_STATUS','2_SV_301_STATUS'\n",
    "    ,'2_SV_401_STATUS','2_SV_501_STATUS','2_SV_601_STATUS','3_LS_001_AL','3_MV_001_STATUS','3_MV_002_STATUS','3_MV_003_STATUS','3_P_001_STATUS','3_P_002_STATUS','3_P_003_STATUS','3_P_004_STATUS','PLANT_START_STOP_LOG',\n",
    "    '2_MV_001_STATUS', '2_MV_002_STATUS','2_MV_004_STATUS', '2_MV_005_STATUS', '2_MV_009_STATUS']\n",
    "\n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = normal_original.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
    "    normal = normal_original[VALID_COLUMNS_IN_TRAIN_DATASET]\n",
    "\n",
    "    # Min-Max Normalize\n",
    "    x = normal.values\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    normal = pd.DataFrame(x_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"normal data shape : \", normal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_normal=normal.values[np.arange(window_size)[None, :] + np.arange(normal.shape[0]-window_size)[:, None]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k70ZFxGs-_7m"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name =FILENUM+'_'+str(BATCH_SIZE)+'_'+str(N_EPOCHS)+'_'+str(hidden_size)\n",
    "w_size=windows_normal.shape[1]*windows_normal.shape[2] # 612\n",
    "z_size=windows_normal.shape[1]*hidden_size #1200\n",
    "\n",
    "windows_normal_train = windows_normal\n",
    "windows_normal_val = windows_normal[int(np.floor(.8 *  windows_normal.shape[0])):int(np.floor(windows_normal.shape[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi9S0SGnDKNc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "    torch.from_numpy(windows_normal_train).float().view(([windows_normal_train.shape[0],w_size]))\n",
    ") , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "    torch.from_numpy(windows_normal_val).float().view(([windows_normal_val.shape[0],w_size]))\n",
    ") , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = UsadModel(w_size, z_size)\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "So9yjDPEDObC",
    "outputId": "629bcd13-37b1-4907-ef0d-46d9e3ad5398",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = training(N_EPOCHS,model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieObNqKYsOzh"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'encoder': model.encoder.state_dict(),\n",
    "            'decoder1': model.decoder1.state_dict(),\n",
    "            'decoder2': model.decoder2.state_dict()\n",
    "            },'usad_hai/'+'usad_front_'+\"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training data prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = UsadModel(w_size, z_size)\n",
    "model = to_device(model,device)\n",
    "\n",
    "checkpoint = torch.load('usad_hai/'+'usad_front_'+\"model.pth\")\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.decoder1.load_state_dict(checkpoint['decoder1'])\n",
    "model.decoder2.load_state_dict(checkpoint['decoder2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1=testing_stacking(model, train_loader, alpha=alpha, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=torch.stack(results1[:-1]).detach().cpu().numpy()\n",
    "y_pred2=results1[-1].detach().cpu().numpy()\n",
    "\n",
    "print('y_pred1.shape',y_pred1.shape)\n",
    "print('y_pred2.shape',y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape1, shape2 = y_pred1.shape[0]*y_pred1.shape[1],y_pred1.shape[2]\n",
    "y_pred1 = y_pred1.reshape(shape1,shape2)\n",
    "print(y_pred1.shape)\n",
    "print(y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usad_pred =np.concatenate([y_pred1,y_pred2])\n",
    "shape1 = windows_normal_train.shape[0]\n",
    "shape2 = windows_normal_train.shape[1]\n",
    "shape3 = windows_normal_train.shape[2]\n",
    "usad_pred = usad_pred.reshape(shape1,shape2,shape3)\n",
    "print(usad_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('usad_hai/usad_swat_train.npy', usad_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymhjbmvR_DgJ"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Load & Merge function\n",
    "def data_load_merge (dataname) :\n",
    "    if dataname == 'HAI' :\n",
    "        VALIDATION_DATASET = sorted([x for x in Path(\"data/HAI 2.0/validation/\").glob(\"*.csv.gz\")])\n",
    "        validation_data = dataframe_from_csvs(VALIDATION_DATASET)\n",
    "\n",
    "    elif dataname == 'SWaT' :\n",
    "        validation_data = pd.read_csv(\"data/SWaT_Dataset_Attack_v0.csv\",sep=\";\")\n",
    "\n",
    "    elif dataname == 'WADI': \n",
    "        validation_data = pd.read_csv(\"data/WADI_attackdataLABLE.csv\", header=1)\n",
    "\n",
    "    return validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "    return pd.concat([dataframe_from_csv(x) for x in targets])\n",
    "\n",
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\anomaly_hai1\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3361: DtypeWarning: Columns (1,9,28,46) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "attack_original= data_load_merge(DATANAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp/ipykernel_5668/1631854668.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  attack[i]=attack[i].apply(lambda x: str(x).replace(\",\" , \".\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# HAI data preprocessing\n",
    "if DATANAME == 'HAI' :\n",
    "    columns = attack_original.columns\n",
    "    DROP_FIELD = [\"time\", \n",
    "                columns[2], columns[3], columns[14], columns[18], columns[19], columns[21], columns[22], columns[25], columns[33], columns[34], columns[35], columns[37], columns[40], columns[43], columns[51], columns[52], columns[59], columns[61], columns[63], columns[64], columns[65], columns[67],\n",
    "                columns[4], columns[5], columns[6], columns[7], columns[8], columns[10], columns[11], columns[17], columns[24], columns[28], columns[32], columns[44], columns[46], columns[48], columns[49], columns[50], columns[53], columns[58], columns[62], columns[71], columns[76], columns[78], columns[79]]\n",
    "\n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = attack_original.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
    "    \n",
    "    TAG_MIN = attack_original[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "    TAG_MAX = attack_original[VALID_COLUMNS_IN_TRAIN_DATASET].max()\n",
    "\n",
    "    # Min-Max Normalize\n",
    "    attack = normalize(attack_original[VALID_COLUMNS_IN_TRAIN_DATASET])\n",
    "\n",
    "\n",
    "# SWaT data preprocessing\n",
    "if DATANAME == 'SWaT' :\n",
    "    columns = attack_original.columns\n",
    "    DROP_FIELD = [\"Timestamp\", \"Normal/Attack\",'P102','P201','P202','P204','P206','P401','P403','P404','P502','P601','P603','P301']\n",
    "  \n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = attack_original.columns.drop(DROP_FIELD)\n",
    "    attack = attack_original[VALID_COLUMNS_IN_TRAIN_DATASET]\n",
    "\n",
    "    # Transform all columns into float64\n",
    "    for i in list(attack): \n",
    "        attack[i]=attack[i].apply(lambda x: str(x).replace(\",\" , \".\"))\n",
    "    attack= attack.astype(float)\n",
    "\n",
    "    x = attack.values\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    attack = pd.DataFrame(x_scaled)\n",
    "\n",
    "# WADI data preprocessing\n",
    "if DATANAME == 'WADI' :\n",
    "    columns = attack_original.columns\n",
    "    DROP_FIELD = [\"Time\", \"Date\",\"Row\", '2_LS_001_AL', '2_LS_002_AL', '2_P_001_STATUS', '2_P_002_STATUS','1_LS_001_AL','1_LS_002_AL','1_MV_002_STATUS','1_MV_003_STATUS','1_P_002_STATUS','1_P_004_STATUS','1_P_006_STATUS','2_P_004_STATUS','2_PIC_003_SP','2_SV_101_STATUS','2_SV_201_STATUS','2_SV_301_STATUS'\n",
    "    ,'2_SV_401_STATUS','2_SV_501_STATUS','2_SV_601_STATUS','3_LS_001_AL','3_MV_001_STATUS','3_MV_002_STATUS','3_MV_003_STATUS','3_P_001_STATUS','3_P_002_STATUS','3_P_003_STATUS','3_P_004_STATUS','PLANT_START_STOP_LOG',\n",
    "    '2_MV_001_STATUS', '2_MV_002_STATUS','2_MV_004_STATUS', '2_MV_005_STATUS', '2_MV_009_STATUS']\n",
    "    TIMESTAMP_FIELD = \"Time\"\n",
    "    ATTACK_FIELD = \"attack\"\n",
    "\n",
    "    VALID_COLUMNS_IN_TRAIN_DATASET = attack_original.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
    "    attack = attack_original[VALID_COLUMNS_IN_TRAIN_DATASET]\n",
    "\n",
    "    # Min-Max Normalize\n",
    "    x = attack.values\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    attack = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449907, 12, 39)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_attack=attack.values[np.arange(window_size)[None, :] + np.arange(attack.shape[0]-window_size)[:, None]]\n",
    "windows_attack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_size=windows_normal.shape[1]*windows_normal.shape[2] # 612\n",
    "#z_size=windows_normal.shape[1]*hidden_size #1200\n",
    "w_size = windows_attack.shape[1]*windows_attack.shape[2]\n",
    "z_size = windows_attack.shape[1]*hidden_size\n",
    "model = UsadModel(w_size, z_size)\n",
    "model = to_device(model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
    "    torch.from_numpy(windows_attack).float().view(([windows_attack.shape[0],w_size]))\n",
    ") , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ry1QTp6V2ny4"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('usad_hai/'+'usad_front_'+\"model.pth\")\n",
    "model.encoder.load_state_dict(checkpoint['encoder'])\n",
    "model.decoder1.load_state_dict(checkpoint['decoder1'])\n",
    "model.decoder2.load_state_dict(checkpoint['decoder2'])\n",
    "results2=testing_stacking(model, test_loader,alpha=alpha, beta=beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FSWwxheNvxR7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred1.shape (219, 2048, 468)\n",
      "y_pred2.shape (1395, 468)\n"
     ]
    }
   ],
   "source": [
    "y_pred1=torch.stack(results2[:-1]).detach().cpu().numpy()\n",
    "y_pred2=results2[-1].detach().cpu().numpy()\n",
    "\n",
    "print('y_pred1.shape',y_pred1.shape)\n",
    "print('y_pred2.shape',y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448512, 468)\n",
      "(1395, 468)\n",
      "BACK 모델 test SWaT data 결과 저장 (449907, 12, 39)\n"
     ]
    }
   ],
   "source": [
    "shape1, shape2 = y_pred1.shape[0]*y_pred1.shape[1],y_pred1.shape[2]\n",
    "y_pred1 = y_pred1.reshape(shape1,shape2)\n",
    "print(y_pred1.shape)\n",
    "print(y_pred2.shape)\n",
    "\n",
    "usad_test_pred =np.concatenate([y_pred1,y_pred2])\n",
    "shape1 = windows_attack.shape[0]\n",
    "shape2 = windows_attack.shape[1]\n",
    "shape3 = windows_attack.shape[2]\n",
    "usad_test_pred = usad_test_pred.reshape(shape1,shape2,shape3)\n",
    "print(\"BACK 모델 test \"+DATANAME+\" data 결과 저장\",usad_test_pred.shape)\n",
    "\n",
    "np.save('usad_hai/usad_swat_test.npy', usad_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "USAD_test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "07557edf3d2e09f90a79224c4df533952df00ef951de9f255089ba71301b579e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anomaly_hai1': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
